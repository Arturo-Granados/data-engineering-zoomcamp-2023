# Week 1: INTRODUCTION AND PREREQUISITES

"Introduction & Prerequisites" is the first part of the course and provides an introduction to data engineering and the necessary prerequisites to complete the course. In this section, students will learn the basic concepts of data engineering. All of this will be accompanied by a practical exercise in data ingestion. For this first week, it will be essential to work with tools such as Python, Docker, and PostgreSQL.

Data ingestion is the process of collecting, processing, and importing raw data from various sources into a system for storage and analysis. It is a critical step in data engineering as it forms the foundation for all downstream processing and analysis of data. Proper data ingestion ensures that data is properly collected, validated, and stored in a way that is accessible and usable for downstream analysis.

Tools such as Docker, Python, and PostgreSQL are essential in data ingestion as they provide the necessary infrastructure and tools for processing and storing large amounts of data. Docker is used for containerization, allowing for efficient deployment and management of data processing pipelines. Python is a powerful programming language that is commonly used for data processing and analysis, with libraries and frameworks such as Pandas, NumPy, and TensorFlow providing robust tools for data manipulation and analysis. PostgreSQL is a relational database management system that is widely used for storing and querying large datasets, making it an ideal choice for data storage and management in data ingestion pipelines.

In summary, data ingestion is a critical step in data engineering, and the use of tools such as Docker, Python, and PostgreSQL is essential in building scalable and efficient data ingestion pipelines.

